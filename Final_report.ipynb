{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from IPython.display import *\n",
    "\n",
    "import AssembleData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anime Trends Based on MyAnimeList\n",
    "\n",
    "**Authors**: Stephen Chen, Sean Reidy, Roger Liu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "**Anime**, or Japanese Animation, commonly refers to animated television shows which originate from Japan. The medium contains shows spanning a vast number of genres, with shows geared towards children, such as *Pokemon*, to content aimed at young adults, such as *Full Metal Alchemist*.\n",
    "\n",
    "![](images/anime.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**MyAnimeList**](https://myanimelist.net/) (abbreviated MAL) is an information database website about anime, much like how *IMDb* is a database for movies. It provides metadata about anime titles, such as synopses, the involved production companies, and genres (e.g. “Action” or “Comedy”). Registred users of MAL can rate and review the animes they’ve watched, which MAL then aggregates into a public score from 1 to 10. As such, MAL serves as a useful indicator of which shows are “good” in the community’s eyes, as well as which shows are “famous.”\n",
    "\n",
    "![](images/mal_example.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the past years, with the rise of the Internet, the worldwide community of anime watchers grew larger and more defined, while the anime industry became increasingly aware of its audience. Tropes, or patterns in storytelling and character archetypes, developed to fit fan expectations. But as these tropes were reused over many titles, viewers became increasingly wary of “cookie-cutter” shows, many of which ended up forgotten in the mass of “average” shows. In this context, certain animation studios made their claim to fame by either breaking traditions or producing similar shows but of better technical quality. Long time watchers of anime eventually developed a natural intuition for which shows would be “average” or “good” just from the shows’ descriptions and production studios. These notions motivated the idea that there are some underlying trends in the anime medium.\n",
    "\n",
    "\n",
    "Our project focused on discovering the underlying patterns of anime and of the anime community by performing data analysis on MAL. We first developed a web scraper which retrieved most of the metadata available on an anime title’s webpage. We then performed data exploration with the purpose of identifying trends within the metadata and exploring relationships between metadata and user scores. We explored trends within each genre, identified patterns within synopsis text, and analyzed the impact of the production studio and source material on popularity. Finally, we addressed the question: “Can we predict the MAL score for an anime title given its metadata?” Our investigation showed that prediction of score is impossible. Instead, we discovered that we can classify which titles are “above average” just from a title’s production studio, genres, source material and length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "We occasionally have personal interpretations of trends in the\n",
    "context of the anime industry and community. Since they are unimportant\n",
    "in the analysis, we have seperated them as \"trivia\" boxes like such.\n",
    "''';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/data_examples.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scraper we wrote pulled information from all shows that were airing between 1998 to 2015. We did this by iterating through each broadcasting season (winter, spring, summer, fall) between those years and pulling metadata for each anime still airing during that time frame. The information was saved as a JSON, giving each title its own file identified by its MAL ID.\n",
    "\n",
    "User contributions to MAL work as follows. Registered users each have an *AnimeList*, which is an individual profile that allows them to keep track of animes they have seen or plan to watch. Each anime entry added to the list can be labelled as “Completed”, “Watching”, “Plan to Watch”, “On-Hold”, and “Dropped”. Users can score an anime on an integer scale of 1 to 10 if the title is listed as “Completed” or “Watching” on their AnimeList. A page on MAL for an anime title consists of two user aggregates: **score** and **members**. Score is the average of scores users have given a particular title. Members is the total number of users who have added the anime to their AnimeList; this includes titles listed under “Plan to Watch”, “On-Hold”, and “Dropped”.\n",
    "\n",
    "Intuitively, score represents how “good” an anime is, as judged by MAL users. Members is an approximation for how “famous” a show is. \n",
    "\n",
    "Following is all data fields that appear in the JSON:\n",
    "\n",
    "![“Fields”](images/fields.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Fields\n",
    "We computed some additional fields during our data exploration:\n",
    "\n",
    "![“More Fields”](images/more_fields.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Cleaning the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build Dataframe from the data/ folder\n",
    "# MAL_df, all_genre  = AssembleData.read_files_all(write_csv=True, verbose=True)\n",
    "# MAL_df = MAL_df.drop_duplicates('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Alternatively save and load to a pickle file for faster loading.\n",
    "import pickle\n",
    "\n",
    "# MAL_df = MAL_df.drop_duplicates('id')\n",
    "# with open('augmented_1998_2015_data.pkl', 'w') as f:\n",
    "#     pickle.dump(MAL_df, f)\n",
    "#     pickle.dump(all_genre, f)\n",
    "\n",
    "with open('augmented_1998_2015_data.pkl', 'r') as f:\n",
    "    MAL_df = pickle.load(f)\n",
    "    all_genre = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sanity Check\n",
    "MAL_df.drop_duplicates('id', inplace=True)\n",
    "\n",
    "# Convert studio list to usable form\n",
    "def fix_studios(l):\n",
    "    \"\"\"\n",
    "        Fixes some the studio names in list l\n",
    "    \"\"\"\n",
    "    # Collapse AIC companies under AIC\n",
    "    # Collapse Xebec Zwei into Xebec\n",
    "    for i in xrange(len(l)):\n",
    "        if l[i].startswith('AIC'):\n",
    "            l[i] = u'AIC'\n",
    "        elif l[i].startswith('Xebec'):\n",
    "            l[i] = u'Xebec'\n",
    "    return l\n",
    "\n",
    "MAL_df['studios'] = MAL_df['studios'].apply(lambda s: eval(s))\n",
    "MAL_df['studios'] = MAL_df['studios'].apply(fix_studios)\n",
    "\n",
    "# Length computation\n",
    "MAL_df['length'] = MAL_df['duration'] * MAL_df['episodes']\n",
    "# Normalize Length to mean 0, std 1 \n",
    "MAL_df['norm_length'] = (MAL_df['length'] - np.mean(MAL_df['length'])) / np.std(MAL_df['length'])\n",
    "\n",
    "# Convert start date to usable form\n",
    "MAL_df['aired_start'] = pd.to_datetime(MAL_df['aired_start'])\n",
    "\n",
    "# Compute bayes_scores\n",
    "MAL_df['norm_score'] = (MAL_df['score'] - 1.0) / 9.0\n",
    "MAL_df['bayes_score'] = (MAL_df['norm_score']*MAL_df['score_users'] + 1.0) / (MAL_df['score_users'] + 2.0)\n",
    "\n",
    "# Finally index it properly.\n",
    "MAL_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The following code block builds a set of all the studios.\n",
    "# split_field is also needed for later code blocks.\n",
    "\n",
    "def split_field(df, field, new_field_name=None):\n",
    "    new_rows = []\n",
    "    for index, row in df.iterrows():\n",
    "        for item in row[field]:\n",
    "            new_rows.append((row['id'], item))\n",
    "            \n",
    "    needs_replace = False\n",
    "    if new_field_name is None or len(new_field_name) == 0:\n",
    "        new_field_name = \"__temp_\" + field\n",
    "        needs_replace = True\n",
    "        \n",
    "    right_df = pd.DataFrame(new_rows, columns=['id', new_field_name])\n",
    "    new_df = pd.merge(df, right_df, how='left', left_on='id', right_on='id')\n",
    "    if needs_replace:\n",
    "        del new_df[field]\n",
    "        new_df[field] = new_df[new_field_name]\n",
    "        del new_df[new_field_name]\n",
    "        \n",
    "    return new_df\n",
    "\n",
    "def build_studios_set(df):\n",
    "    split = split_field(df[['id', 'studios']], 'studios')\n",
    "    split.dropna(axis=0, subset=['studios'], inplace=True)\n",
    "    return split['studios'].unique()\n",
    "\n",
    "all_studios = build_studios_set(MAL_df)\n",
    "print \"Number of studios:\", all_studios.shape[0]\n",
    "all_studios[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The following code block builds the set of all sources\n",
    "\n",
    "def build_source_set(df):\n",
    "    data = df['source'].copy()\n",
    "    data.dropna(axis=0, inplace=True)\n",
    "    return data.unique()\n",
    "\n",
    "all_sources = build_source_set(MAL_df)\n",
    "print \"Number of sources:\", all_sources.shape[0]\n",
    "all_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The list of genres are loaded with the pickle file.\n",
    "# Check to make sure there are 44 genres here.\n",
    "print len(all_genre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Initial Exploratory Data Analysis\n",
    "\n",
    "After we collected our data into an organized pandas dataframe, we began the process of exploratory data analysis; where we looked at the various distributions of both the categorical variables and continuous variables. \n",
    "\n",
    "This process provided necessary insights into the effect the an works genre had on it’s respective score distribution. As depicted by the  histogram below, genres including psychological, shounen, and drama have score distributions that skew  It was evident that towards higher scores compared to genres like kids, where the distribution exhibits a heavy tail of low scores. From this we gathered that the genre was a definitive feature in potentially estimating the expected score of a work. \n",
    "\n",
    "We are most interested in the user score of a given anime, as we gather it's a good metric for the quality of an anime.  The scores follow a unimodal distribution that is roughly normal centered approximately around 7.5 out of 10. This distribution exhibits a tail towards the lower scores, evinced of some low scoring entries pulling down the mean. \n",
    "\n",
    "![\"Score Den  Plot](Rcode/scoreDen.png)\n",
    "\n",
    "This scatter plot matrix illustrates the bivariate relationships among all the continuous variables in the data frame. Where each cell in the matrix is a scatter plot with its x and y vars represented by its position.  One interesting correlation is that longer length in a show seems to imply higher scores. This is likely because companies only invest in producing a movie when they know it will be well received.\n",
    "\n",
    "![Scatterplot Mat](Rcode/scatterplotMatrix.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score and Members\n",
    "Scores and member count are intuitively correlated. The better a show, the more people are likely to have heard about it (and add it to their AnimeList)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = (14.0, 9.0)\n",
    "plt.scatter(MAL_df['members'], MAL_df['score'])\n",
    "plt.xlabel(\"members\")\n",
    "plt.ylabel(\"score\")\n",
    "plt.title('score vs members')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our graphic here shows a few interesting points. The major point related to our analysis is that there are a few data points which hit score exactly 0 or 10. This poses the problem that titles with lower number of users ratings can be heavily skewed. We address this in later analysis by using a new “bayes_score” field which normalizes the score to [0.0, 1.0] and adds an additional score of 0 and 1 to balance out the low number of scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = (14.0, 9.0)\n",
    "plt.scatter(MAL_df['members'], MAL_df['bayes_score'])\n",
    "plt.xlabel(\"members\")\n",
    "plt.ylabel(\"bayes score\")\n",
    "plt.title('bayes score vs members')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Curiously, there seems to be a score threshold around 7.0 (.667 bayes_score): if an anime does not score at least 7.0, it will never become majorly popular on MAL. Alternatively, if an anime is popular on MAL, it should have a score of at least 7.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "There are a few interesting outlier which have high member count despite low scores.\n",
    "These are School Days and Pupa, which are notoriously violent animes.\n",
    "The incredible violence has drawn much attention despite their lack of quality. \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Genre Analysis\n",
    "\n",
    "![“Genre user plot”](images/genre_show_score_dist.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Deviations in Genres\n",
    "\n",
    "![“Genre std”](images/genre_top_std.png) ![“Genre std”](images/genre_bot_std.png)\n",
    "\n",
    "\n",
    "![“Genre user plot”](images/genre_user_score_dist.png)\n",
    "Something interesting to note is that while the distribution of scores among shows is fairly normal, the distribution amoung users is strongly heavy tailed. Also interesting to note is that the genres which we found to be the most highly rated have a significantly large set of 9 and 10 scores rather than simply a peak of 8’s and 9’s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impact of Genre on Score\n",
    "\n",
    "Before even reading the synopsis of a show, a viewer will want to know what **genre** the show is in. The while the genres are intended to be a straightforward description of a shows story, many genre carry loaded definitions due to other popular shows in that genre. For example, the tag **shonen**, which signifies that the show is intented for the boy's demographic, has many tropes associated with it due to the popularity of shows like DragonBall and One Piece. \n",
    "\n",
    "Observing this, we did a 1st pass linear regression on the genre tags against score and member count\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "    <td> <img src=\"images/genre_score_reg.png\" width=\"200\"/> </td>\n",
    "    <td> <img src=\"images/genre_member_reg.png\" width=\"200\"/> </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Though our R^2 value was around 0.2, the fact that the tags mostly aligned with our notions of generally \"good\" genres suggested that there was a link between genre and score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impact of Studios on Score\n",
    "\n",
    "The *studios* field consists of the animation studio(s) that are primarily responsible for creating an anime. Some studios are recognizable by name in the community either because of how many titles they have released or how well received their titles have been.\n",
    "\n",
    "Our data set features animes from 302 unique studios after collapsing branches of companies into their parent branch. Our first step was to rank the studios by the average scores of the animes they released."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Some animes have more than one studio listed.\n",
    "# Need to split each studio into its individual row.\n",
    "split = split_field(MAL_df, 'studios')\n",
    "by_studios = split.groupby('studios', as_index=False)\n",
    "\n",
    "# Wrestling with merges in Pandas.\n",
    "bigger_studios = by_studios.filter(lambda data: len(data) > 2)\n",
    "bigger_studios_groups = bigger_studios.groupby('studios', as_index=False)\n",
    "bigger_studios_size = bigger_studios_groups.size().to_frame('num_titles')\n",
    "bigger_studios_scores_members = bigger_studios_groups[['score','members']].agg(np.mean)\n",
    "bigger_studios_members_sorted = bigger_studios.sort_values('members', ascending=False).groupby('studios')\n",
    "bigger_studios_max = bigger_studios_members_sorted['members'].max().to_frame('max')\n",
    "bigger_studios_2nd = bigger_studios_members_sorted.nth(1)['members'].to_frame('2nd')\n",
    "bigger_studios_3rd = bigger_studios_members_sorted.nth(2)['members'].to_frame('3rd')\n",
    "merged = pd.merge(bigger_studios_scores_members, bigger_studios_size, how='inner', right_index=True, left_on='studios')\n",
    "merged = pd.merge(merged, bigger_studios_max, how='left', left_on='studios', right_index=True)\n",
    "merged = pd.merge(merged, bigger_studios_2nd, how='left', left_on='studios', right_index=True)\n",
    "merged = pd.merge(merged, bigger_studios_3rd, how='left', left_on='studios', right_index=True)\n",
    "\n",
    "\n",
    "# Display the top 20 scoring studios.\n",
    "merged = merged.sort_values(['score', 'members'], ascending=False)\n",
    "merged = merged.reset_index(drop=True)\n",
    "merged[['studios', 'score', 'members', 'num_titles']].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results were not within our personal experience. This is because studios often produce small animations to advertise their mainline series, and these short episodes are separate entries on MAL. We decided to filter these short entries out because they do not represent a studio as well as their main counterparts. We thus removed all entries whose total length was less than 30 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Specifically, many of these are Original Video Animations (OVAs)\n",
    "which are special, unaired single episodes of an anime shipped\n",
    "with the anime’s DVD release.  These typically score worse than\n",
    "their parent title on MAL since they generally do not advance\n",
    "the parent story’s plot.\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get only entries that have total length greater than 30 minutes\n",
    "sig_length = MAL_df[MAL_df['length'] > 30]\n",
    "print \"Total entries:\", len(MAL_df)\n",
    "print \"Significant:\", len(sig_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split each studio into its own row\n",
    "split = split_field(sig_length, 'studios')\n",
    "by_studios = split.groupby('studios', as_index=False)\n",
    "\n",
    "# Wrestle with merges and groups in Pandas\n",
    "bigger_studios = by_studios.filter(lambda data: len(data) > 2)\n",
    "bigger_studios_groups = bigger_studios.groupby('studios', as_index=False)\n",
    "bigger_studios_size = bigger_studios_groups.size().to_frame('num_titles')\n",
    "bigger_studios_scores_members = bigger_studios_groups[['score','members']].agg(np.mean)\n",
    "bigger_studios_members_sorted = bigger_studios.sort_values('members', ascending=False).groupby('studios')\n",
    "bigger_studios_max = bigger_studios_members_sorted['members'].max().to_frame('max')\n",
    "bigger_studios_2nd = bigger_studios_members_sorted.nth(1)['members'].to_frame('2nd')\n",
    "bigger_studios_3rd = bigger_studios_members_sorted.nth(2)['members'].to_frame('3rd')\n",
    "merged = pd.merge(bigger_studios_scores_members, bigger_studios_size, how='inner', right_index=True, left_on='studios')\n",
    "merged = pd.merge(merged, bigger_studios_max, how='left', left_on='studios', right_index=True)\n",
    "merged = pd.merge(merged, bigger_studios_2nd, how='left', left_on='studios', right_index=True)\n",
    "merged = pd.merge(merged, bigger_studios_3rd, how='left', left_on='studios', right_index=True)\n",
    "\n",
    "# Display top 20 studios\n",
    "merged = merged.sort_values(['score', 'members'], ascending=False)\n",
    "merged = merged.reset_index(drop=True)\n",
    "merged[['studios', 'score', 'members', 'num_titles']].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was more in line with our expectations. Particularly interesting are the studios which have high average score despite producing more than 5 titles, such as *Studio Ghibli* (average 7.93 over 13 titles), *White Fox* (average 7.816 over 13 titles), and *Kyoto Animation* (average 7.774 over 42 titles). This indicates consistent quality from these studios, and suggests that the scores of their productions can be inferred just based on the prestige of the studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Curiously, this is not exactly the list of names that fans\n",
    "in the anime community will immediately recognize. A better\n",
    "representation of renown is captured by the membership.\n",
    "Specifically, of the studio’s produced titles, the title with\n",
    "the second largest membership is a good representation of renown.\n",
    "Intuitively, most fans will only start recognizing a studio’s name\n",
    "after it has produced at least two well known titles. When sorting\n",
    "the studios in fashion, the top 20 list quickly becomes saturated\n",
    "with commonly recognized studios.\n",
    "'''\n",
    "\n",
    "# Sort by studio's anime with the 2nd largest membership\n",
    "merged = merged.sort_values(['2nd'], ascending=False)\n",
    "merged = merged.reset_index(drop=True)\n",
    "merged[['studios', 'score', 'members', 'num_titles']].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impact of Source Material on Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some animes are created directly for television. Many others are based on some source material, such as an existing manga or novel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_source_features(df, source_names):\n",
    "    new_df = df[['id', 'source']].copy()\n",
    "    for source in source_names:\n",
    "        new_df[source] = new_df['source'].apply(lambda s: s == source).astype(float)\n",
    "    return new_df[source_names]\n",
    "\n",
    "# Generate a DataFrame with boolean indicators of Source attached.\n",
    "with_sources = pd.merge(MAL_df, make_source_features(MAL_df, all_sources), how='inner', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the score frequencies by Source Material.\n",
    "matplotlib.rcParams['figure.figsize'] = (30.0, 40.0)\n",
    "i = 1\n",
    "for source in all_sources:\n",
    "    plt.subplot(6, 3, i)\n",
    "    plt.hist(with_sources[with_sources[source] == 1.0]['score'], bins=20, range=[0.0,10.0])\n",
    "    plt.xlabel('score')\n",
    "    plt.ylabel('count')\n",
    "    plt.title('score frequency of \"%s\"' % source)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our histogram tells us that anime original shows tend to hover around 7.0, shows based on *manga* (Japanese comics) or *light novels* (chapter books) hover higher than 7.0, and shows based on *visual novels* (interactive storytelling video games) generally sit below 7.0. This variation is significant considering that scores from 6.7 to 8.0 account for about 45% of all titles. Thus, source material is likely a factor in predicting MAL score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Contextually, we interpret the correlation as follows. Light novels\n",
    "adaptations are highly rated because each novel in the original series\n",
    "tells a complete, self-sustaining story. This translates well into\n",
    "the short timeframe of 13-episode anime adaptation, as these adaptations\n",
    "can usually end with all plot points addressed and without cliffhangers,\n",
    "which leaves viewers satisfied with the anime. Meanwhile, manga\n",
    "adaptations often occur because the source manga is already popular.\n",
    "Thus the anime is also likely to be well received. In contrast,\n",
    "visual novels adaptation tend to suffer because they remove the interactive\n",
    "aspect of the original storytelling game. Moreover, visual novels title\n",
    "typically are not as popular as mangas. Companies only produce these\n",
    "adaptations because the fanbase, although small, is usually dedicated\n",
    "and willing to invest in subsequent merchandise.\n",
    "''';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synopsis Text Analysis \n",
    "\n",
    "##### Code for this section was written in R and code can be found in apendix \n",
    "\n",
    "![\"Main Word Cloud”](Rcode/wordclouds/all_genre.png)\n",
    "\n",
    "\n",
    "The synopsis of a given work provides a brief explanation of setting, plot and character details that a viewer should expect in the anime.  The free form nature of this text could lead to a powerful tool for classifying different works into clusters and groups based of the vocabulary used within each synopsis.  We built a large document term matrix of all the synopses separated into different documents by their genre. After removing  punctuation,  numbers/symbols, and commonly used english words (stop words), we created a frequency table of the more common words.\n",
    "\n",
    "![\"Doc Term Mat\"](images/documentTermMat2.png)\n",
    "\n",
    "Among the most frequently used words across all the genres included:  one, world, will, new ,school, life, however, girl, friends, two, day, and now. This alludes to larger tropes and trends commonly found in anime, for example where the setting is a “school” and the story’s twist is predicated with the word “however” \n",
    "\n",
    "![\"Word Frequency\"](images/wordFreq.png)\n",
    "\n",
    "Perhaps the most interesting results from this were the word associations found within the document term matrix. For example, the word, “school” was closely related to the words “boys”, “doesn't”, “student” and “high”.  \n",
    "\n",
    "\n",
    "Separating by genre, we find that there are some defining words that represent some genres, but this proved to be rather inconclusive as synopsis tend to be similar across all the works. \n",
    "\n",
    "#### Action \n",
    "\n",
    "![\"Action Word Cloud\"](Rcode/wordclouds/MAL_action .png)\n",
    "\n",
    "#### Shounen \n",
    "\n",
    "![\"Shounen Word Cloud\"](Rcode/wordclouds/MAL_shounen .png)\n",
    "\n",
    "#### Slice Of Live \n",
    "\n",
    "![\"Slice of Life Word Cloud\"](Rcode/wordclouds/MAL_sliceoflife .png)\n",
    "\n",
    "More word clouds for each genre can be found here.  \n",
    "https://github.com/FourSwordKirby/MALDataScience/tree/master/Rcode/wordclouds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting Sequels and Related Entries into Their Parent Title\n",
    "When perusing through various shows on MyAnimeList, we found that many shows had sequels and related works listed underneath them. In addition, we noticed that multiple works from the same overall series ended up near the top of our various metrics. With this in mind, we decided to see what would happen if we were to works into their singular parent series. \n",
    "\n",
    "\n",
    "To do this, we took all of the works we collected and sorted them by number of members. This would make it so that series were denoted by their most popular work.We then iterated through this list. For each work, we recursively found the id’s of its the related works to form a series that encapsulated all of those works. We put these series into a into a new dataframe. A show was deemed to be related if it’s parent has a has a link pointing to it and it has a link pointing to its parent. We made sure to never include the id of a show that already belonged to another series.\n",
    "\n",
    "\n",
    "In collapsing the data we found the following results. The number of entries dropped from __7053__ to __3915__\n",
    "<table>\n",
    "<tr>\n",
    "    <td> <img src=\"images/series_count_30.png\" width=\"300\"/> </td>\n",
    "    <td> <img src=\"images/series_score_30.png\" width=\"300\"/> </td>\n",
    "</tr>\n",
    "</table>\n",
    "When removing all “insignificant” works (defined previously) we get the following. The number of entries dropped from __4853__ to __3121__\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "    <td> <img src=\"images/series_filter_count_30.png\" width=\"300/\"> </td>\n",
    "    <td> <img src=\"images/series_filter_score_30.png\" width=\"300\"/> </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "The degree to which shows were collapsed is not too surprising, as it suggests that about 1 in 3 works gets a second season, movie, or a significant set of DVD extras. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning MAL Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features\n",
    "Our main motivation was to determine if we can predict an anime’s score based on its metadata.\n",
    "\n",
    "Based on our data exploration, we determined that a title’s total length, genres, studios, and source material are all well correlated with its MAL score. Comparatively, our synopsis text analysis demonstrated the synopsis was indicative of the genre, but not necessarily of the quality of the title. We deemed this a reasonable conclusion when we manually read the synopses, as many high scoring titles had synopses similar to those of low scoring titles.\n",
    "\n",
    "Our learning features thus included 363 features consisting of:\n",
    "\n",
    "* An indicator variable {0, 1} for each 302 possible studios.\n",
    "* An indicator variable {0, 1} for each of the 44 genres.\n",
    "* An indicator variable {0, 1} for each of the 16 source material types.\n",
    "* `norm_length` = title’s total length normalized to mean 0, standard deviation 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_studio_features(df, studio_names):\n",
    "    new_df = df[['id', 'studios']].copy()\n",
    "    for studio in studio_names:\n",
    "        new_df[studio] = new_df['studios'].apply(lambda l: studio in l).astype(float)\n",
    "    return new_df[studio_names]\n",
    "\n",
    "def make_source_features(df, source_names):\n",
    "    new_df = df[['id', 'source']].copy()\n",
    "    for source in source_names:\n",
    "        new_df[source] = new_df['source'].apply(lambda s: s == source).astype(float)\n",
    "    return new_df[source_names]\n",
    "\n",
    "def make_features(df):\n",
    "    p1 = make_studio_features(df, all_studios)\n",
    "    p2 = df[all_genre + ['norm_length']]\n",
    "    p3 = make_source_features(df, all_sources)\n",
    "    join = pd.concat([p1, p2, p3], axis=1, join='inner')\n",
    "    return join\n",
    "\n",
    "make_features(MAL_df.head(5)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression\n",
    "\n",
    "\n",
    "We initially began with fitting a Linear Regression to these features to predict the MAL score. Our preliminary results were not promising, as the fits produced coefficients of correlation that were either incredible low or absurd values. This was likely because of the abundance of categorical variables, which translated into boolean indicator variables of 0 or 1, making accurate fits impossible. However, the learned parameters of these models showed that the high scoring genres, studios, and source materials that we discovered during data exploration did tend to have larger parameters values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying “Above Average”\n",
    "\n",
    "\n",
    "We then simplified our question. Rather than predicting MAL scores exactly, we wanted to predict whether an anime was “above average” or “below average.” Anime enthusiast can intuitively make this prediction based on an anime’s metadata and description, as they can typically identify “cookie-cutter” shows which fall into the “average” or “below average” spectrum. This motivated our belief that learning algorithms could do the same.\n",
    "\n",
    "\n",
    "Our data was split into a training, validation, and test set based on the starting air date: before 2012 for training, 2012-2014 for validation, and 2015 for test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def split_data_by_date(df):\n",
    "    df = df.sort_values('aired_start')\n",
    "    t_date = pd.to_datetime('2012-01-01')\n",
    "    v_date = pd.to_datetime('2015-01-01')\n",
    "    train = df[df['aired_start'] < t_date]\n",
    "    val   = df[(df['aired_start'] >= t_date) & (df['aired_start'] < v_date)]\n",
    "    test  = df[df['aired_start'] >= v_date]\n",
    "    return train, val, test\n",
    "\n",
    "data = MAL_df\n",
    "\n",
    "train_df, val_df, test_df = split_data_by_date(data)\n",
    "print \"available\", data.shape[0]\n",
    "print \"train\", train_df.shape[0]\n",
    "print \"validation\", val_df.shape[0]\n",
    "print \"test\", test_df.shape[0]\n",
    "\n",
    "X_tr = make_features(train_df)\n",
    "X_cv = make_features(val_df)   \n",
    "X_test = make_features(test_df)\n",
    "\n",
    "print \"X_tr\", X_tr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scores we used for this section is the `bayes_score` field, which is the MAL score normalized to [0.0,1.0] with an extra 0 and 1 rating. We interpreted “above average” in two ways: first, above median scores across all of the entries MAL, and second, above a MAL score of 7.2 for practical purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Above Median\n",
    "For this situation, we labeled the entries according to whether its score was above or below the median MAL score (6.74 unnormalized, 0.637 normalized)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "median_bayes_score = np.median(MAL_df['bayes_score'])\n",
    "print \"Median bayes_score:\", median_bayes_score\n",
    "print \"Median Score\", median_bayes_score * 9 + 1\n",
    "\n",
    "def make_labels(df):\n",
    "    return df['bayes_score'] > median_bayes_score\n",
    "\n",
    "y_tr = make_labels(train_df)\n",
    "y_cv = make_labels(val_df)\n",
    "y_test = make_labels(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `sklearn`’s `GradientBoostingClassifier`, we scored 73.5% accuracy on the validation set, which we consider a large success over a baseline of randomly guessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "clf = GradientBoostingClassifier()\n",
    "clf.fit(X_tr, y_tr)\n",
    "\n",
    "score = clf.score(X_cv, y_cv)\n",
    "print \"score\", score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also tried Logistic Regression with hyperparameter `C = 1.8`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "\n",
    "def get_err_reg(clf, X, y, X_cv, y_cv, C):\n",
    "    clf.C = C\n",
    "    clf.fit(X, y)\n",
    "    return np.array([clf.score(X, y), clf.score(X_cv, y_cv)])\n",
    "\n",
    "C = np.logspace(-3, 4, 50)\n",
    "errors = np.array([get_err_reg(clf, X_tr, y_tr, X_cv, y_cv, c) for c in C])\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (10.0, 7.0)\n",
    "plt.semilogx(C, errors[:,0], C, errors[:,1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression(C=1.8) # Use this when considering all data, not just extremities\n",
    "clf.fit(X_tr, y_tr)\n",
    "print clf.score(X_tr, y_tr)\n",
    "print clf.score(X_cv, y_cv)\n",
    "print clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our classifier scored 75.7% accuracy on the 2015 test set. These test results broke down into 78.0% accuracy on predicting above median anime, and 73.4% accuracy on predicting below median anime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Accuracy on positives\n",
    "pos_idx = y_test > 0\n",
    "print \"Positive accuracy\", clf.score(X_test[pos_idx], y_test[pos_idx])\n",
    "\n",
    "neg_idx = y_test < 1\n",
    "print \"Negative accuracy\", clf.score(X_test[neg_idx], y_test[neg_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Above Score 7.2\n",
    "Practically, predicting above median is not worthwhile since the majority of the anime community has never heard of shows with scores below the median of 6.74 (evidenced by [large membership in animes which score above 7.0](#Score-and-Members)). In practice, it is more useful to know if an anime scores above average of commonly known shows. Our personal experiences argued that a MAL score of 7.2 more accurately represents a “practical average.” This translates to the 70th percentile, which is reasonable when assuming that most known shows score approximately above the median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The following code block prints the percentile of works per category\n",
    "intuitive_thresholds = [\n",
    "    ('barely_watchable', 6.7),\n",
    "    ('average', 7.2),\n",
    "    ('above_average', 7.6),\n",
    "    ('recommendable', 8.0),\n",
    "    ('amazing', 8.5),\n",
    "    ('gods', 9.0)\n",
    "]\n",
    "\n",
    "def to_bayes_score(score):\n",
    "    return ((score - 1.0) / 9.0)\n",
    "\n",
    "def percentages(df):\n",
    "    print \"Using regular score\"\n",
    "    for label, val in intuitive_thresholds:\n",
    "        print \"{} ({}): {}\".format(\n",
    "            label,\n",
    "            val,\n",
    "            1. - float(sum(df['score'] > val)) / df.shape[0])\n",
    "        \n",
    "    print \"\"\n",
    "    print \"Using bayes scores\"\n",
    "    for label, val in intuitive_thresholds:\n",
    "        thresh = ((val - 1.0) / 9.0)\n",
    "        print \"{} ({}): {}\".format(\n",
    "            label,\n",
    "            thresh,\n",
    "            1. - float(sum( df['bayes_score'] > thresh )) / df.shape[0])\n",
    "        \n",
    "def percentage_bayes_score(df, score):\n",
    "    return float(sum( df['bayes_score'] > score )) / df.shape[0]\n",
    "\n",
    "print \"==== Percentiles of Titles ====\"\n",
    "percentages(MAL_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_labels(df):\n",
    "    return df['bayes_score'] > to_bayes_score(7.2) # ~70 percentile\n",
    "\n",
    "y_tr = make_labels(train_df)\n",
    "y_cv = make_labels(val_df)\n",
    "y_test = make_labels(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running a `GradientBoostingClassifier`, we scored 78.6% on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "clf = GradientBoostingClassifier()\n",
    "clf.fit(X_tr, y_tr)\n",
    "\n",
    "score = clf.score(X_cv, y_cv)\n",
    "print \"score\", score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we tried Logistic Regression with hyperparameter `C = 1.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "\n",
    "def get_err_reg(clf, X, y, X_cv, y_cv, C):\n",
    "    clf.C = C\n",
    "    clf.fit(X, y)\n",
    "    return np.array([clf.score(X, y), clf.score(X_cv, y_cv)])\n",
    "\n",
    "C = np.logspace(-3, 4, 50)\n",
    "errors = np.array([get_err_reg(clf, X_tr, y_tr, X_cv, y_cv, c) for c in C])\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (10.0, 7.0)\n",
    "plt.semilogx(C, errors[:,0], C, errors[:,1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression(C=1.) # Use this when considering all data for above/below 70 percentile\n",
    "clf.fit(X_tr, y_tr)\n",
    "print clf.score(X_tr, y_tr)\n",
    "print clf.score(X_cv, y_cv)\n",
    "print clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We scored 79.8% on the test set, which breaks down into 59.8% accuracy on above practical average animes, and 88.5% accuracy on below practical average animes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Accuracy on positives\n",
    "pos_idx = y_test > 0\n",
    "print \"Positive accuracy\", clf.score(X_test[pos_idx], y_test[pos_idx])\n",
    "\n",
    "neg_idx = y_test < 1\n",
    "print \"Negative accuracy\", clf.score(X_test[neg_idx], y_test[neg_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is ~10 percentage points better accuracy than always predicting below average, and significantly better than random guessing.\n",
    "\n",
    "\n",
    "When examining the Logistic Regression coefficients, we confirmed our original expectation that high scoring studios, genres, and source material would have a large influence on prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_names = np.array(X_tr.columns.values)\n",
    "feature_coeff = pd.DataFrame(sorted(zip(feature_names, clf.coef_[0]), key=lambda x : np.abs(x[1]), reverse=True),\n",
    "                             columns=['Feature Name', 'Coefficient'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_coeff[feature_coeff['Feature Name'].apply(lambda s: s in all_studios)].head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_coeff[feature_coeff['Feature Name'].apply(lambda s: s in all_sources)].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of Learned Classifier\n",
    "\n",
    "A majority of shows are properly classified, as evident by a visible division at score 7.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "colors = np.array([\n",
    "            [1, 0, 0],\n",
    "            [0, 0, 1],\n",
    "            [0, 0, 0]\n",
    "        ])\n",
    "\n",
    "classes = pd.Series(clf.predict(make_features(MAL_df)).astype(int))\n",
    "#classes[MAL_df['hentai'] == 1] = 2\n",
    "colored_classes = colors[classes]\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (14.0, 9.0)\n",
    "plt.scatter(MAL_df['members'], MAL_df['score'], c=colored_classes)\n",
    "plt.xlabel(\"members\")\n",
    "plt.ylabel(\"score\")\n",
    "plt.title('score vs members')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This charts the scores of animes as they first aired over the years.\n",
    "# The size of each dot is proportional to the number of members an anime has.\n",
    "# NOTE: The time axis cannot be plotted as a date.\n",
    "#       The time span stretches from 1998 -> 2016 though.\n",
    "newer_shows = MAL_df[MAL_df['aired_start'] > pd.to_datetime('1997')]\n",
    "\n",
    "colors = np.array([\n",
    "            [1, 0, 0],\n",
    "            [0, 0, 1],\n",
    "            [.8, .8, .8]\n",
    "        ])\n",
    "classes = pd.Series(clf.predict(make_features(newer_shows)).astype(int))\n",
    "colored_classes = colors[classes]\n",
    "\n",
    "sizes = (np.log(newer_shows['members']) + 1) * 5\n",
    "\n",
    "x_axis = newer_shows['aired_start'].apply(matplotlib.dates.date2num)\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (14.0, 9.0)\n",
    "plt.scatter(x_axis, newer_shows['score'], c=colored_classes, s=sizes)\n",
    "plt.xlabel(\"starting air date\")\n",
    "plt.ylabel(\"score\")\n",
    "plt.title('scores of anime released from 1998-2015')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "There a few curious outliers, specifically the “above average”\n",
    "show with high member count and low score that aired recently. \n",
    "This is \"Pupa\", as previously mentioned, an incredibly violent show.\n",
    "\"Pupa\" was probably classified highly because of its psychological\n",
    "genre and its owning studio Studio Deen, which is well known for some\n",
    "other works. Ironically, the show was originally highly anticipated,\n",
    "but quickly fell through the rankings as the content turned out far\n",
    "below expectation. This conflicts between metadata and actual content\n",
    "seems to have been reflected in our classifier.\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Feel free to try seeing what the classifer does on cerntain titles.\n",
    "title_to_predict = 'Amagi Brilliant Park'\n",
    "clf.decision_function(make_features(MAL_df[MAL_df['title'] == title_to_predict]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "![outliers](images/outliers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work:(Sean add more)\n",
    "   \n",
    "Through our analysis, we were able to discover interesting trends that underlie anime and the community that surround it. In addition, the classifier we developed was able to identify good anime from bad anime. Future work that can be done in this field would be to utilize the vast amounts of user recommendations and communities that exist on MyAnimeList. By leveraging these data sources, we could potentially figure out which works are derivative and which works are “original”. In addition, MyAnimeList also stores information about the characters that appear in shows, allowing users to add them to a list of favorites. By leveraging the information stored in these profiles, we could potentially determine traits that make characters “popular”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
